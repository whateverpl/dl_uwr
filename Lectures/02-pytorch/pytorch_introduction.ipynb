{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.3.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "torch.Size([5, 4])\n",
      "\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "(5, 4)\n"
     ]
    }
   ],
   "source": [
    "# Similar to numpy\n",
    "x_torch = torch.ones(5, 4)\n",
    "print(x_torch)\n",
    "print(x_torch.shape) #alias for x.size()\n",
    "print()\n",
    "y_np = np.ones((5,4))\n",
    "print(y_np)\n",
    "print(y_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rand: tensor([[4.0064e-01, 1.6530e-02],\n",
      "        [3.1573e-04, 3.2196e-01]])\n",
      "from_list: tensor([[1., 2.],\n",
      "        [4., 5.]])\n",
      "element wise: tensor([[4.0064e-01, 3.3060e-02],\n",
      "        [1.2629e-03, 1.6098e+00]])\n",
      "matrix multiplication:  tensor([[0.4668, 0.8839],\n",
      "        [1.2881, 1.6104]])\n"
     ]
    }
   ],
   "source": [
    "# Rand\n",
    "rand = torch.rand(2,2)\n",
    "print('rand:', rand)\n",
    "# From List\n",
    "from_list = torch.tensor([[1.,2.], [4., 5.]])\n",
    "print('from_list:', from_list)\n",
    "x = rand * from_list\n",
    "print('element wise:', x)\n",
    "y = rand @ from_list\n",
    "print('matrix multiplication: ', y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arange: tensor([[0],\n",
      "        [1]])\n",
      "res: tensor([[1., 2.],\n",
      "        [5., 6.]])\n"
     ]
    }
   ],
   "source": [
    "#  broadcasting also works as expected\n",
    "arange = torch.arange(2)[:, None]\n",
    "print('arange:', arange)\n",
    "res = from_list + arange\n",
    "print('res:', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 4.],\n",
       "        [7., 8.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in place\n",
    "res.add_(2)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any operation that mutates a tensor in-place is post-fixed with an ``_``.\n",
    "    For example: ``x.copy_(y)``, ``x.t_()``, will change ``x``.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1., 1.]]])\n",
      "[[[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1. 1. 1.]\n",
      "  [1. 1. 1. 1. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "#But some functionalities are diffrent\n",
    "#You have been warned\n",
    "x_torch = x_torch.view(2, 2, -1)\n",
    "print(x_torch)\n",
    "\n",
    "y_np = y_np.reshape(2, 2, -1)\n",
    "print(y_np)\n",
    "\n",
    "# In torch there is also `reshape` function. But it beahves difrentlly.\n",
    "# It sometimes can make a copy of an array instead of a reference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a one element tensor, use ``.item()`` to get the value as a\n",
    "Python number\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3955])\n",
      "0.3955190181732178 <class 'float'>\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1)\n",
    "print(x)\n",
    "print(x.item(), type(x.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy bridge\n",
    "Converting a Torch Tensor to a NumPy array and vice versa is a breeze.\n",
    "Transformation do not need copying underlying C array. Only `python` properties of the object has to be changed\n",
    "\n",
    "### Conclussion: It is fast\n",
    "Sometimes it is easier to transofrm data in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(5)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "b = a.numpy()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how the numpy array changed in value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 2., 2., 2., 2.])\n",
      "[2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "a.add_(1)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting NumPy Array to Torch Tensor\n",
    "\n",
    "See how changing the np array changed the Torch Tensor automatically\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 2. 2. 2. 2.]\n",
      "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "a = np.ones(5)\n",
    "b = torch.from_numpy(a)\n",
    "np.add(a, 1, out=a)\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the Tensors on the CPU except a CharTensor support converting to\n",
    "NumPy and back.\n",
    "\n",
    "# CUDA Tensors\n",
    "\n",
    "\n",
    "Tensors can be moved onto any device using the ``.to`` method. To interact with each other both tensors have to be on the same device. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.3955], device='cuda:0')\n",
      "tensor([1.3955], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# let us run this cell only if CUDA is available\n",
    "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")          # a CUDA device object\n",
    "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
    "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
    "    z = x + y\n",
    "    print(z)\n",
    "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended way for comapatibillity\n",
    "\n",
    "On systems with multiple GPU's there will be {`cuda:1`, `cuda:2`...} and so on.\n",
    "\n",
    "There is no automatic way to decide which GPU is \"free\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "tensor = torch.zeros(2,2).to(device)\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'RuntimeError'> expected device cpu but got device cuda:0\n"
     ]
    }
   ],
   "source": [
    "cpu = torch.ones(1)\n",
    "try:\n",
    "    cpu + tensor\n",
    "except Exception as e:\n",
    "    print(type(e), e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Autograd: Automatic Differentiation\n",
    "===================================\n",
    "\n",
    "The ``autograd`` package provides automatic differentiation for all operations\n",
    "on Tensors. It is a define-by-run framework, which means that your backprop is\n",
    "defined by how your code is run, and that every single iteration can be\n",
    "different.\n",
    "\n",
    "\n",
    "Tensor\n",
    "--------\n",
    "\n",
    "1. If you set its attribute``.requires_grad`` as ``True``, it starts to track all operations on it. \n",
    "\n",
    "2. When\n",
    "you finish your computation you can call ``.backward()`` and have all the\n",
    "gradients computed automatically. The gradient for this tensor will be\n",
    "accumulated into ``.grad`` attribute.\n",
    "\n",
    "3. To stop a tensor from tracking history, you can call ``.detach()`` to detach\n",
    "it from the computation history, and to prevent future computation from being\n",
    "tracked.\n",
    "\n",
    "4. To prevent tracking history (and using memory), you can also wrap the code block\n",
    "in ``with torch.no_grad():``. This can be particularly helpful when **evaluating a\n",
    "model** because the model may have trainable parameters with\n",
    "``requires_grad=True``, but for which we don't need the gradients.\n",
    "\n",
    "There’s one more class which is very important for autograd\n",
    "implementation - a ``Function``.\n",
    "\n",
    "``Tensor`` and ``Function`` are interconnected and build up an acyclic\n",
    "graph, that encodes a complete history of computation. Each tensor has\n",
    "a ``.grad_fn`` attribute that references a ``Function`` that has created\n",
    "the ``Tensor`` (except for Tensors created by the user - their\n",
    "``grad_fn is None``).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a tensor and set ``requires_grad=True`` to track computation with it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 3, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a tensor operation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]], grad_fn=<AddBackward0>)\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y`` was created as a result of an operation, so it has a ``grad_fn``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7fd25690a4e0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do more operations on ``y``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27., 27.],\n",
      "        [27., 27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``.requires_grad_( ... )`` changes an existing Tensor's ``requires_grad``\n",
    "flag in-place. The input flag defaults to ``False`` if not given.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 2)\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients\n",
    "---------\n",
    "Let's backprop now.\n",
    "Because ``out`` contains a single scalar, ``out.backward()`` is\n",
    "equivalent to ``out.backward(torch.tensor(1.))``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print gradients d(out)/dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take a look at an example of vector-Jacobian product:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[102., 102.],\n",
      "        [102., 102.]])\n",
      "tensor([[142., 142.],\n",
      "        [142., 142.]])\n"
     ]
    }
   ],
   "source": [
    "x_1 = torch.ones(2,2, requires_grad=True)\n",
    "x_2 = (x_1 ** 2 * 3).sum()\n",
    "x_2.backward(torch.tensor(17.))\n",
    "print(x_1.grad)\n",
    "x_3 = (x_1 * 20).sum()\n",
    "x_3.backward(torch.tensor(2.))\n",
    "print(x_1.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also stop autograd from tracking history on Tensors\n",
    "with ``.requires_grad=True`` either by wrapping the code block in\n",
    "``with torch.no_grad():``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "\tprint((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or by using ``.detach()`` to get a new Tensor with the same\n",
    "content but that does not require gradients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "y = x.detach()\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Neural Networks\n",
    "\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.\n",
    "\n",
    "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
    "``autograd`` to define models and differentiate them.\n",
    "An ``nn.Module`` contains layers, and a method ``forward(input)``\\ that\n",
    "returns the ``output``.\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "\n",
    "\n",
    "It is a simple feed-forward network. It takes the input, feeds it\n",
    "through several layers one after the other, and then finally gives the\n",
    "output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "Define the network\n",
    "------------------\n",
    "\n",
    "Let’s define this network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=1024, out_features=200, bias=True)\n",
      "  (fc2): Linear(in_features=200, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "# torch modules\n",
    "import torch.nn as nn\n",
    "# torch funct\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operations: y = Wx + b\n",
    "        self.fc1 = nn.Linear(32 * 32, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x.shape => BATCH_SIZE x Height X Width \n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return x \n",
    "    \n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the ``forward`` function, and the ``backward``\n",
    "function (where gradients are computed) is automatically defined for you\n",
    "using ``autograd``.\n",
    "You can use any of the Tensor operations in the ``forward`` function.\n",
    "\n",
    "The learnable parameters of a model are returned by ``net.parameters()``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([200, 1024])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # fc1 .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a random 32x32 input.\n",
    "Note: expected input size of this net (LeNet) is 32x32. To use this net on\n",
    "the MNIST dataset, please resize the images from the dataset to 32x32.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2206, 0.0917, 0.0000, 0.0000, 0.1850, 0.2828, 0.0000, 0.3440, 0.0000,\n",
      "         0.1580]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random\n",
    "gradients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
    "    package only supports inputs that are a mini-batch of samples, and not\n",
    "    a single sample.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Loss Function\n",
    "-------------\n",
    "A loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6399, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you follow ``loss`` in the backward direction, using its\n",
    "``.grad_fn`` attribute, you will see a graph of computations that looks\n",
    "like this:\n",
    "\n",
    "\n",
    "\n",
    "    input -> view -> linear -> relu -> linear -> relu -> linear\n",
    "          -> MSELoss\n",
    "          -> loss\n",
    "\n",
    "So, when we call ``loss.backward()``, the whole graph is differentiated\n",
    "w.r.t. the loss, and all Tensors in the graph that has ``requires_grad=True``\n",
    "will have their ``.grad`` Tensor accumulated with the gradient.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop\n",
    "--------\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at linear bias\n",
    "gradients before and after the backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "fc1.bias.grad after backward\n",
      "tensor([-0.0223,  0.0000,  0.0000,  0.0000,  0.0000, -0.0090,  0.0000,  0.0086,\n",
      "         0.0000,  0.0416,  0.0000,  0.0000,  0.0139,  0.0000,  0.0000,  0.0175,\n",
      "         0.0000,  0.0000, -0.0105,  0.0000,  0.0000,  0.0000, -0.0170,  0.0000,\n",
      "         0.0043, -0.0211,  0.0059,  0.0062,  0.0000,  0.0000, -0.0021,  0.0000,\n",
      "         0.0000,  0.0000, -0.0128,  0.0039,  0.0160,  0.0000,  0.0207,  0.0095,\n",
      "        -0.0324, -0.0169,  0.0131,  0.0216,  0.0356,  0.0000, -0.0159, -0.0369,\n",
      "         0.0000,  0.0031,  0.0011,  0.0000,  0.0000,  0.0000,  0.0000,  0.0062,\n",
      "         0.0000,  0.0093, -0.0065,  0.0179, -0.0245,  0.0332,  0.0000,  0.0000,\n",
      "         0.0000, -0.0003,  0.0281,  0.0000,  0.0000,  0.0299,  0.0000, -0.0440,\n",
      "         0.0000,  0.0000,  0.0066,  0.0000,  0.0000,  0.0016,  0.0233,  0.0005,\n",
      "         0.0000,  0.0000,  0.0160,  0.0006,  0.0193,  0.0061, -0.0128, -0.0064,\n",
      "         0.0272, -0.0006,  0.0009,  0.0000,  0.0000,  0.0000,  0.0000, -0.0109,\n",
      "        -0.0076, -0.0471,  0.0000,  0.0284,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0136,  0.0010,  0.0000, -0.0114,  0.0256,  0.0008,\n",
      "         0.0258,  0.0000, -0.0028,  0.0037,  0.0129,  0.0000,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000,  0.0156,  0.0000,  0.0264,  0.0310,  0.0000,  0.0000,\n",
      "         0.0000,  0.0000, -0.0040,  0.0000,  0.0000,  0.0000,  0.0229,  0.0073,\n",
      "         0.0000,  0.0000, -0.0141,  0.0000,  0.0034,  0.0050,  0.0000,  0.0000,\n",
      "         0.0000, -0.0103,  0.0000,  0.0381, -0.0124,  0.0088,  0.0000,  0.0061,\n",
      "         0.0240,  0.0000, -0.0208,  0.0000,  0.0147,  0.0000, -0.0305,  0.0000,\n",
      "         0.0000, -0.0184,  0.0000,  0.0000,  0.0040,  0.0154, -0.0177,  0.0364,\n",
      "         0.0000,  0.0206, -0.0089, -0.0024, -0.0234,  0.0318, -0.0191,  0.0000,\n",
      "         0.0000,  0.0000,  0.0000,  0.0355,  0.0000,  0.0000,  0.0000,  0.0299,\n",
      "        -0.0220,  0.0000,  0.0000,  0.0000,  0.0157,  0.0000, -0.0037, -0.0153,\n",
      "         0.0000,  0.0000,  0.0133, -0.0125, -0.0256, -0.0080,  0.0000,  0.0000])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('fc1.bias.grad before backward')\n",
    "print(net.fc1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('fc1.bias.grad after backward')\n",
    "print(net.fc1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update the weights\n",
    "------------------\n",
    "The simplest update rule used in practice is the Stochastic Gradient\n",
    "Descent (SGD):\n",
    "\n",
    " ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "We can implement this using simple Python code:\n",
    "\n",
    "```\n",
    "learning_rate = 0.01\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)\n",
    "```\n",
    "However, as you use neural networks, you want to use various different\n",
    "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "To enable this, we built a small package: ``torch.optim`` that\n",
    "implements all these methods. Using it is very simple:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preparing a dataset\n",
    "\n",
    "A lot of effort in solving any machine learning problem goes in to preparing the data. PyTorch provides many tools to make data loading easy and hopefully, to make your code more readable.\n",
    "\n",
    "Dataset class\n",
    "-------------\n",
    "\n",
    "``torch.utils.data.Dataset`` is an abstract class representing a\n",
    "dataset.\n",
    "Your custom dataset should inherit ``Dataset`` and override the following\n",
    "methods:\n",
    "\n",
    "-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n",
    "-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n",
    "   be used to get $i$'th sample\n",
    "\n",
    "\n",
    "Sample of our dataset will be a dict\n",
    "``{'image': image, 'label': label}``.\n",
    "\n",
    "### my_SimpleDataset\n",
    "\n",
    "Dataset containes images build form zeroes and ones.\n",
    "Our machine_learning task is to figure out if we have **more ones than zeros** in the picture. (This task is trivial)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_len: 8192\n",
      "{'image': tensor([[1., 1., 1., 0., 0., 1., 1., 0., 1., 1.],\n",
      "        [0., 0., 1., 0., 0., 1., 1., 1., 1., 0.],\n",
      "        [0., 0., 1., 0., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 1.],\n",
      "        [1., 0., 1., 1., 1., 1., 0., 1., 0., 1.],\n",
      "        [0., 1., 1., 1., 1., 1., 1., 1., 0., 1.],\n",
      "        [0., 1., 0., 0., 1., 0., 1., 0., 0., 1.],\n",
      "        [1., 1., 1., 1., 0., 1., 1., 0., 0., 1.],\n",
      "        [1., 1., 0., 1., 0., 1., 1., 0., 0., 1.],\n",
      "        [1., 0., 0., 1., 0., 0., 1., 1., 1., 0.]]), 'label': tensor(1.)}\n",
      "looping sucesfull\n"
     ]
    }
   ],
   "source": [
    "PICTURE_SIZE = 10\n",
    "\n",
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.data = torch.zeros(size, PICTURE_SIZE, PICTURE_SIZE)\n",
    "        self.data[torch.rand_like(self.data) > 0.5] = 1.\n",
    "        \n",
    "        self.labels = torch.zeros(size)\n",
    "        self.labels[self.data.mean(dim=[1,2]) > 0.5] = 1.\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        # Accepts scalars, tuples and dictionaries\n",
    "        return {\n",
    "                'image': self.data[item],\n",
    "                'label': self.labels[item]\n",
    "        }\n",
    "    def __len__(self):\n",
    "        return self.data.shape[0]\n",
    "\n",
    "dataset = SimpleDataset(2**13)\n",
    "# __len__\n",
    "print('dataset_len:', len(dataset))\n",
    "\n",
    "# __getitem__\n",
    "print(dataset[15])\n",
    "\n",
    "for x in dataset:\n",
    "    pass\n",
    "print('looping sucesfull')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we are losing a lot of features by using a simple ``for`` loop to\n",
    "iterate over the data. In particular, we are missing out on:\n",
    "\n",
    "-  Batching the data\n",
    "-  Shuffling the data\n",
    "-  Load the data in parallel using ``multiprocessing`` workers.\n",
    "\n",
    "``torch.utils.data.DataLoader`` is an iterator which provides all these\n",
    "features. Parameters used below should be clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NeuralNet definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%% \n"
    }
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 800\n",
    "\n",
    "class Net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net2, self).__init__()\n",
    "        self.hidden = nn.Sequential(\n",
    "                nn.Linear(PICTURE_SIZE * PICTURE_SIZE, HIDDEN_SIZE),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(HIDDEN_SIZE, 1)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, PICTURE_SIZE * PICTURE_SIZE)\n",
    "        x = self.hidden(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   1 step:    900 loss: 207.521 acc: 0.379\n",
      "epoch:   1 step:   1800 loss: 17.871 acc: 0.370\n",
      "epoch:   1 step:   2700 loss: 17.141 acc: 0.464\n",
      "epoch:   1 step:   3600 loss: 16.798 acc: 0.467\n",
      "epoch:   2 step:    900 loss: 16.733 acc: 0.517\n",
      "epoch:   2 step:   1800 loss: 15.717 acc: 0.537\n",
      "epoch:   2 step:   2700 loss: 15.542 acc: 0.543\n",
      "epoch:   2 step:   3600 loss: 15.276 acc: 0.577\n",
      "epoch:   3 step:    900 loss: 15.188 acc: 0.656\n",
      "epoch:   3 step:   1800 loss: 14.561 acc: 0.654\n",
      "epoch:   3 step:   2700 loss: 14.214 acc: 0.648\n",
      "epoch:   3 step:   3600 loss: 13.831 acc: 0.694\n",
      "epoch:   4 step:    900 loss: 13.513 acc: 0.745\n",
      "epoch:   4 step:   1800 loss: 12.247 acc: 0.753\n",
      "epoch:   4 step:   2700 loss: 11.523 acc: 0.760\n",
      "epoch:   4 step:   3600 loss: 11.064 acc: 0.773\n",
      "epoch:   5 step:    900 loss: 10.788 acc: 0.804\n",
      "epoch:   5 step:   1800 loss: 9.585 acc: 0.810\n",
      "epoch:   5 step:   2700 loss: 9.390 acc: 0.809\n",
      "epoch:   5 step:   3600 loss: 11.080 acc: 0.781\n",
      "epoch:   6 step:    900 loss: 9.428 acc: 0.832\n",
      "epoch:   6 step:   1800 loss: 9.940 acc: 0.773\n",
      "epoch:   6 step:   2700 loss: 8.929 acc: 0.815\n",
      "epoch:   6 step:   3600 loss: 9.147 acc: 0.813\n",
      "epoch:   7 step:    900 loss: 8.510 acc: 0.869\n",
      "epoch:   7 step:   1800 loss: 9.393 acc: 0.819\n",
      "epoch:   7 step:   2700 loss: 9.455 acc: 0.785\n",
      "epoch:   7 step:   3600 loss: 9.697 acc: 0.815\n",
      "epoch:   8 step:    900 loss: 10.071 acc: 0.835\n",
      "epoch:   8 step:   1800 loss: 7.740 acc: 0.853\n",
      "epoch:   8 step:   2700 loss: 10.276 acc: 0.785\n",
      "epoch:   8 step:   3600 loss: 8.148 acc: 0.836\n",
      "epoch:   9 step:    900 loss: 8.802 acc: 0.854\n",
      "epoch:   9 step:   1800 loss: 7.691 acc: 0.880\n",
      "epoch:   9 step:   2700 loss: 9.173 acc: 0.803\n",
      "epoch:   9 step:   3600 loss: 7.784 acc: 0.852\n",
      "epoch:  10 step:    900 loss: 8.749 acc: 0.857\n",
      "epoch:  10 step:   1800 loss: 8.510 acc: 0.821\n",
      "epoch:  10 step:   2700 loss: 7.859 acc: 0.838\n",
      "epoch:  10 step:   3600 loss: 6.882 acc: 0.882\n",
      "epoch:  11 step:    900 loss: 7.509 acc: 0.898\n",
      "epoch:  11 step:   1800 loss: 8.279 acc: 0.828\n",
      "epoch:  11 step:   2700 loss: 7.818 acc: 0.859\n",
      "epoch:  11 step:   3600 loss: 7.041 acc: 0.866\n",
      "epoch:  12 step:    900 loss: 6.824 acc: 0.919\n",
      "epoch:  12 step:   1800 loss: 7.001 acc: 0.868\n",
      "epoch:  12 step:   2700 loss: 8.617 acc: 0.822\n",
      "epoch:  12 step:   3600 loss: 7.490 acc: 0.854\n",
      "epoch:  13 step:    900 loss: 6.772 acc: 0.916\n",
      "epoch:  13 step:   1800 loss: 7.368 acc: 0.857\n",
      "epoch:  13 step:   2700 loss: 7.901 acc: 0.863\n",
      "epoch:  13 step:   3600 loss: 7.099 acc: 0.882\n",
      "epoch:  14 step:    900 loss: 6.714 acc: 0.909\n",
      "epoch:  14 step:   1800 loss: 6.743 acc: 0.882\n",
      "epoch:  14 step:   2700 loss: 5.162 acc: 0.939\n",
      "epoch:  14 step:   3600 loss: 6.638 acc: 0.889\n",
      "epoch:  15 step:    900 loss: 6.228 acc: 0.944\n",
      "epoch:  15 step:   1800 loss: 6.066 acc: 0.899\n",
      "epoch:  15 step:   2700 loss: 5.355 acc: 0.931\n",
      "epoch:  15 step:   3600 loss: 6.196 acc: 0.899\n",
      "epoch:  16 step:    900 loss: 4.897 acc: 0.986\n",
      "epoch:  16 step:   1800 loss: 7.206 acc: 0.866\n",
      "epoch:  16 step:   2700 loss: 5.589 acc: 0.916\n",
      "epoch:  16 step:   3600 loss: 5.070 acc: 0.936\n",
      "epoch:  17 step:    900 loss: 6.358 acc: 0.939\n",
      "epoch:  17 step:   1800 loss: 5.373 acc: 0.927\n",
      "epoch:  17 step:   2700 loss: 5.763 acc: 0.913\n",
      "epoch:  17 step:   3600 loss: 5.415 acc: 0.926\n",
      "epoch:  18 step:    900 loss: 5.110 acc: 0.972\n",
      "epoch:  18 step:   1800 loss: 5.133 acc: 0.928\n",
      "epoch:  18 step:   2700 loss: 5.162 acc: 0.930\n",
      "epoch:  18 step:   3600 loss: 5.250 acc: 0.935\n",
      "epoch:  19 step:    900 loss: 4.426 acc: 0.994\n",
      "epoch:  19 step:   1800 loss: 4.850 acc: 0.936\n",
      "epoch:  19 step:   2700 loss: 5.025 acc: 0.936\n",
      "epoch:  19 step:   3600 loss: 6.292 acc: 0.891\n",
      "epoch:  20 step:    900 loss: 4.009 acc: 1.007\n",
      "epoch:  20 step:   1800 loss: 4.774 acc: 0.947\n",
      "epoch:  20 step:   2700 loss: 5.065 acc: 0.940\n",
      "epoch:  20 step:   3600 loss: 4.764 acc: 0.947\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "# Move model weigths to device\n",
    "device = ('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "net = Net2().to(device)\n",
    "\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "    loss_avg = 0.\n",
    "    acc_avg = 0.\n",
    "    for i, batch_data in (enumerate(dataloader)):\n",
    "        # Start with zeroing .grad fields in model\n",
    "        net.zero_grad()\n",
    "        \n",
    "        # Move data to device\n",
    "        images = batch_data['image'].to(device)\n",
    "        labels = batch_data['label'].to(device)\n",
    "        \n",
    "        #Run neural net\n",
    "        out = net(images)\n",
    "\n",
    "        #Compute loss and apply autograd for model parameters\n",
    "        loss = ((out.view(-1) - labels)** 2).sum()\n",
    "        loss.backward()\n",
    "        \n",
    "        #Update network weigths\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics generation\n",
    "        res = torch.where(\n",
    "                out.view(-1) > 0.5,\n",
    "                torch.tensor(1., device=device),\n",
    "                torch.tensor(0., device=device)\n",
    "        )\n",
    "        acc = (res == labels).sum()\n",
    "        loss_avg += loss\n",
    "        acc_avg += acc\n",
    "        if i % 30 == 0 and i != 0:\n",
    "            print(f'epoch: {e + 1:3d} step: {(i * 30):6d} loss: {loss_avg / 30:.3f} acc: {acc_avg / (BATCH_SIZE * 30):.3f}')\n",
    "            acc_avg = 0.\n",
    "            loss_avg = 0.\n",
    "        # End of statistics generation\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "source": [
    "Used materials:\n",
    "* Deep Learning with PyTorch: A 60 Minute Blitz https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html\n",
    "* Writing Custom Datasets, DataLoaders and Transforms https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
